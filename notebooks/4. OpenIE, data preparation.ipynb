{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract from: https://1drv.ms/u/s!ApPZx_TWwibImHl49ZBwxOU0ktHv\n",
    "# available in data/neural_oie\n",
    "# Or even better the \"span model\" dataset for \n",
    "# https://arxiv.org/abs/1901.10879\n",
    "import re\n",
    "import json\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import h5py\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "data_path = Path('../data') / 'span_model_oie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need ~6 GB of RAM to load the entire JSON in memory\n",
    "with open(data_path / 'structured_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "* treshold at 0.9 score, because some of the lower scored training entries are quite poor (e.g. \"Descartes\" used as relation, score 0.39)\n",
    "* the start of each input should be alphanumeric, as well as both ends of the triple fragments\n",
    "* we'll ignore position indexes, and POS tags, as we will let the WordPiece tokenizer retokenize anything, and we are (currently) not interesting with creating pointers between input string and output triple\n",
    "* also contain the extraction to some \"common sense\" approximation of a typical noun phrase. E.g. at most 7 words in either subject or object entry. \n",
    "   * Example of what we want to avoid:\n",
    "     ```\n",
    "      'a 17th-18th century french writer whose defensio religionis a 251-page critique of the pantheism of john toland'\n",
    "     ```\n",
    "   * and what we'd like to instead keep:\n",
    "     ```\n",
    "      a 17th-18th century french writer\n",
    "     ```\n",
    "   * valid maximal length example: (*third son of a humble farmer*, **will be a part of**, *a much anticipated next generation reality show*)\n",
    "   * as we don't have a truly \"stable\" way to vet the training data ourselves, we'll simply discard the extra long annotations to avoid that kind of noise. We are also more interested in triples that are closer to \"ontological\", so best to avoid confounding training.\n",
    "* another cleaning condition - drop relations that do not contain at least one word that appears in the unix dictionary (e.g. eliminating names that wrongly appear as relations -- too much noise!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/usr/share/dict/words','r') as dict_f:\n",
    " unix_words = set(dict_f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphanum_start = re.compile('^[\\W_]+', re.UNICODE)\n",
    "alphanum_end =  re.compile('[\\W_]+$', re.UNICODE)\n",
    "def sanitize(w):\n",
    "    w = re.sub(alphanum_start, '', w)\n",
    "    w = re.sub(alphanum_end, '', w)\n",
    "    return w\n",
    "\n",
    "filtered_data = []\n",
    "for datum in data:\n",
    "    sentence = sanitize(datum['sentence'])\n",
    "    for triple in datum['tuples']:\n",
    "        # one more constraint - the relation *must* be in the unix dictionary, to drop obvious noise\n",
    "        if triple['score'] > 0.9:\n",
    "            subj = sanitize(triple['arg0'])\n",
    "            pred = sanitize(triple['relation'])\n",
    "            pred_words = pred.split(' ')\n",
    "            if subj.count(' ')<=5 and \\\n",
    "               len(pred_words)<=5 and \\\n",
    "               set(pred_words).issubset(unix_words):\n",
    "                for arg2 in triple['args']:\n",
    "                    obj = sanitize(arg2)\n",
    "                    if obj.count(' ')<=6:\n",
    "                        filtered_data.append([sentence, [subj, pred, obj]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triples: 1002626\n",
      "max lengths: sentence=40, target=17 (subject=6, predicate=5, object=7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total triples: %d\" % len(filtered_data))\n",
    "\n",
    "max_sentence_len = max([len(sent[0].split()) for sent in filtered_data]) \n",
    "max_subj_len = max([len(sent[1][0].split()) for sent in filtered_data])\n",
    "max_pred_len = max([len(sent[1][1].split()) for sent in filtered_data])\n",
    "max_obj_len = max([len(sent[1][2].split()) for sent in filtered_data])\n",
    "max_target_len = max([len(sent[1][0].split())+len(sent[1][1].split())+len(sent[1][2].split()) for sent in filtered_data])\n",
    "\n",
    "print(\"max lengths: sentence=%d, target=%d (subject=%d, predicate=%d, object=%d)\" % \\\n",
    "      (max_sentence_len, max_target_len, max_subj_len, max_pred_len, max_obj_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer to Language Modeling paradigm\n",
    "\n",
    "We would like to use this data in order to fine-tune a language model pretrained on a vast collection of English. In the ideal case, this will allow the underlying transformer architecture the opportunity to generalize the triple extraction procedure over a larger/open-ended subset of English than present in the training data.\n",
    "\n",
    "Deciding on the neural architecture influences how to prepare the final serialization of this data, and pose it as a fine-tuning downstream task to the LM.\n",
    "\n",
    "For now, we are going with:\n",
    " * Use the `huggingface/transformer` library for reliability and simplicity\n",
    " * Currently the only convenient \"out of the box\" experience for summarization/translation appears to be their `\n",
    " * Follow their fine-tuning documentation for BART: [README](https://github.com/huggingface/transformers/blob/master/examples/seq2seq/README.md)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We ideally have a grasp of our expected max-sizes, so that we can use input/output shapes with fixed lengths\n",
    "max_input_size = 128\n",
    "max_output_size = 64\n",
    "distil_bert = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(distil_bert)\n",
    "\n",
    "### The following check was used to initially estimate max_input_size prior setting \"max_length\":\n",
    "### And similarly for max_output_size\n",
    "# max_encoded_len = max([len(sent[0]) for sent in encoded_input]) \n",
    "# print(\"max encoded sentence len: %d\" % max_encoded_len)\n",
    "# max_encoded_target_len = max([len(sent[0]) for sent in encoded_target]) \n",
    "# print(\"max encoded target len: %d\" % max_encoded_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is quite RAM intensive as well, likely more than reasonable, but works on my beefy PC\n",
    "# I am seeing ~15 GB of RAM allocated for creating `encoded_input`\n",
    "encoded_input = [tokenizer(datum[0], return_tensors='tf', max_length=max_input_size, padding='max_length')\\\n",
    "                 for datum in filtered_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and another ~9 GB allocated for `encoded_target`\n",
    "encoded_target = [tokenizer(entry[1][0] + \" [SEP] \"+entry[1][1] +\" [SEP] \"+entry[1][2],\\\n",
    "                            return_tensors='tf', max_length=max_output_size, padding='max_length')\\\n",
    "                  for entry in filtered_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To check on some decoded strings use e.g.\n",
    "#print(tokenizer.decode(encoded_input[180000]['input_ids'][0]))\n",
    "#print(tokenizer.decode(encoded_target[180000]['input_ids'][0]))\n",
    "\n",
    "# Grab 10,000 triples for quick prototyping with the modeling setup:\n",
    "sample_input_train = encoded_input[180_000:190_000]\n",
    "sample_targets_train = encoded_target[180_000:190_000]\n",
    "# And 2,500 triples for quick testing, to keep with an 80/20 ratio\n",
    "sample_input_test = encoded_input[780_000:782_500]\n",
    "sample_targets_test = encoded_target[780_000:782_500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook guard - close fp if it is defined, to be able to re-run this cell at any point.\n",
    "if 'fp' in locals():\n",
    "    fp.close()\n",
    "# Write *SAMPLE* data out as an HDF5 dataset\n",
    "fp = h5py.File(data_path / \"encoded_sample.hdf5\", \"w\")\n",
    "train_chunk_size = len(sample_input_train)\n",
    "test_chunk_size = len(sample_input_test)\n",
    "\n",
    "x_train = fp.create_dataset(\"x_train\", (train_chunk_size, max_input_size), dtype=\"int\")\n",
    "y_train = fp.create_dataset(\"y_train\", (train_chunk_size,max_output_size),dtype=\"int\")\n",
    "x_test = fp.create_dataset(\"x_test\", (test_chunk_size, max_input_size), dtype=\"int\")\n",
    "y_test = fp.create_dataset(\"y_test\", (test_chunk_size,max_output_size), dtype=\"int\")\n",
    "\n",
    "for index, sample in enumerate(sample_input_train):\n",
    "    x_train[index,:] = sample['input_ids']\n",
    "for index, sample in enumerate(sample_targets_train):\n",
    "    y_train[index,:] = sample['input_ids']\n",
    "for index, sample in enumerate(sample_input_test):\n",
    "    x_test[index,:] = sample['input_ids']\n",
    "for index, sample in enumerate(sample_targets_test):\n",
    "    y_test[index,:] = sample['input_ids']\n",
    "    \n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write *FULL* data out as an HDF5 dataset\n",
    "if 'fullw' in locals():\n",
    "    fullw.close()\n",
    "fullw = h5py.File(data_path / \"encoded_span_oie.hdf5\", \"w\")\n",
    "full_chunk_size = 100_000\n",
    "input_len = len(encoded_input)\n",
    "train_size=int(0.8 * input_len)\n",
    "test_size=input_len - train_size\n",
    "\n",
    "x_train = fullw.create_dataset(\"x_train\", (train_size, max_input_size), \n",
    "                              chunks=(full_chunk_size, max_input_size), dtype=\"int\")\n",
    "y_train = fullw.create_dataset(\"y_train\", (train_size,max_output_size), \n",
    "                              chunks=(full_chunk_size,max_output_size), dtype=\"int\")\n",
    "x_test = fullw.create_dataset(\"x_test\", (test_size, max_input_size), \n",
    "                              chunks=(full_chunk_size, max_input_size), dtype=\"int\")\n",
    "y_test = fullw.create_dataset(\"y_test\", (test_size,max_output_size), \n",
    "                              chunks=(full_chunk_size,max_output_size), dtype=\"int\")\n",
    "\n",
    "train_idx = 0\n",
    "test_idx = 0\n",
    "for index, (x,y) in enumerate(zip(encoded_input,encoded_target)):\n",
    "    if index % 5 == 0:\n",
    "        x_test[test_idx,:] = x['input_ids']\n",
    "        y_test[test_idx,:] = y['input_ids']\n",
    "        test_idx+=1\n",
    "    else:\n",
    "        x_train[train_idx,:] = x['input_ids']\n",
    "        y_train[train_idx,:] = y['input_ids']\n",
    "        train_idx+=1\n",
    "fullw.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
