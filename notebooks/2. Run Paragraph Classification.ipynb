{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# running with installed tensorflow-gpu 2.3.0, need compat to v1\n",
    "import tensorflow.compat.v1 as tf\n",
    "import keras\n",
    "gpu_options = tf.GPUOptions(\n",
    "    per_process_gpu_memory_fraction=0.95, allow_growth=False)\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=16,\n",
    "                        inter_op_parallelism_threads=16, allow_soft_placement=True, gpu_options=gpu_options)\n",
    "session = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data/mathoverflow')\n",
    "with open(data_dir / 'mathoverflow.json') as json_file: \n",
    "    data = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusing the original statement classification paper at:\n",
    "# https://github.com/dginev/arxiv-statement-classification\n",
    "# via local paths for now:\n",
    "statement_repo_path = Path.home() / \"git/arxiv-statement-classification\"\n",
    "\n",
    "# Load the indexed vocabulary compatible with the model\n",
    "with open(statement_repo_path / 'data/word_index.json') as json_data:\n",
    "    word_index = json.load(json_data)\n",
    "    \n",
    "def embed_text(text,maxlen = 480):\n",
    "    # lowercase, split by non-word chars\n",
    "    words = re.split('[-\\W]+', text.lower())\n",
    "    # map words to indexes and cap at maxlen\n",
    "    indexed = []\n",
    "    current = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            w_ind = word_index[w]\n",
    "            indexed.append(w_ind)\n",
    "            current+=1\n",
    "            if current>=maxlen:\n",
    "                break\n",
    "        except KeyError:\n",
    "            continue\n",
    "            # nothing to do \n",
    "    npad = maxlen-len(indexed)\n",
    "    indexed = np.array(indexed)\n",
    "    if npad > 0:\n",
    "        indexed = np.pad(indexed, (0, npad), mode='constant')\n",
    "    return indexed.reshape(1,480)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_label = [\"abstract\",\"acknowledgement\",\"conclusion\",\"definition\",\"example\",\"introduction\",\"keywords\",\n",
    "                  \"problem\",\"proof\",\"proposition\",\"related_work\",\"remark\",\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_model_path = statement_repo_path / \"models/confusion_bilstm128_batch128_cat13_gpu_notebook.h5\"\n",
    "model = keras.models.load_model(statement_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded input shape:  (266441, 480)\n",
      "2082/2082 [==============================] - 140s 67ms/step\n"
     ]
    }
   ],
   "source": [
    "batch_size=128\n",
    "embedded = np.ndarray(shape=(len(data),480), dtype=int)\n",
    "# This approach needs ~8 GB of RAM for the MathOverflow data\n",
    "for index,datum in enumerate(data):\n",
    "    embedded[index] = embed_text(datum['text'])\n",
    "print(\"embedded input shape: \", embedded.shape)\n",
    "\n",
    "# ~2 minutes of runtime to classify 270k paragraphs on a 1080 TI card.\n",
    "predictions = model.predict(embedded, verbose=1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,(datum,pred) in enumerate(zip(data,predictions)):\n",
    "    pred_label_id = np.argmax(pred, axis=-1)\n",
    "    datum['statement_label'] = index_to_label[pred_label_id]\n",
    "    datum['statement_confidence'] = str(pred[pred_label_id])\n",
    "    data[index] = datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir / 'mathoverflow_statements.json',\"w\") as json_out_file: \n",
    "      json.dump(data, json_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
