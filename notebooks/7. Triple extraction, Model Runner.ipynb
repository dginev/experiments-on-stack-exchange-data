{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertModel, DistilBertConfig\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from pathlib import Path\n",
    "data_path = Path('../data') / 'span_model_oie'\n",
    "model_path = Path('../models')\n",
    "path_hdf5 = str(data_path/'encoded_span_oie.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data sizes: x_train (802100, 128), y_train (802100, 64), x_test (200526, 128), y_test (200526, 64) .\n"
     ]
    }
   ],
   "source": [
    "# Let's quickly get the shapes from HDF5 for bookkeeping\n",
    "if 'fp' in locals():\n",
    "    fp.close()\n",
    "fp = h5py.File(path_hdf5, \"r\")\n",
    "x_train = fp['x_train']\n",
    "x_test = fp['x_test']\n",
    "y_train = fp['y_train']\n",
    "y_test = fp['y_test']\n",
    "x_train_shape = x_train.shape\n",
    "y_train_shape = y_train.shape\n",
    "x_test_shape = x_test.shape\n",
    "y_test_shape = y_test.shape\n",
    "fp.close()\n",
    "print(\"data sizes: x_train %s, y_train %s, x_test %s, y_test %s .\" % \\\n",
    "      (x_train_shape, y_train_shape, x_test_shape, y_test_shape))\n",
    "validation_index = 1+int(0.9*x_train_shape[0])\n",
    "\n",
    "x_test = tfio.IODataset.from_hdf5(path_hdf5, dataset='/x_test')\n",
    "y_test = tfio.IODataset.from_hdf5(path_hdf5, dataset='/y_test')\n",
    "x_train = tfio.IODataset.from_hdf5(path_hdf5, dataset='/x_train')\n",
    "y_train = tfio.IODataset.from_hdf5(path_hdf5, dataset='/y_train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Thanks to the excellent tutorial at:\n",
    "# https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
    "# Setup the config and embedding layer, then prep data.\n",
    "distil_bert = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(distil_bert)\n",
    "max_input_size = 128\n",
    "max_target_size = 64\n",
    "\n",
    "config = DistilBertConfig(dropout=0.2, attention_dropout=0.2, trainable=False)\n",
    "config.output_hidden_states = False\n",
    "transformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_token (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_token (InputLayer)       [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_1 (TFDisti ((None, 128, 768),)  66362880    input_token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, None, 30522) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 128), (None, 459264      tf_distil_bert_model_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 128),  15693312    input_4[0][0]                    \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 30522)  3937338     lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 86,452,794\n",
      "Trainable params: 20,089,914\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the full model.\n",
    "# The crucial part for our setup is \"teacher forcing\", so as to properly teach the full triple generation\n",
    "# Great starting example at: https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py#L159\n",
    "vocab_size = tokenizer.vocab_size\n",
    "num_encoder_tokens = num_decoder_tokens = vocab_size\n",
    "latent_dim = int(max_input_size)\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(max_input_size,), name='input_token', dtype='int32')\n",
    "encoder_masks  = tf.keras.layers.Input(shape=(max_input_size,), name='masked_token', dtype='int32')\n",
    "\n",
    "lm_embedding = transformer_model(encoder_inputs, attention_mask=encoder_masks)[0]\n",
    "encoder = tf.keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(lm_embedding)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                      initial_state=encoder_states)\n",
    "decoder_dense = tf.keras.layers.Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = tf.keras.Model([encoder_inputs, encoder_masks, decoder_inputs], decoder_outputs)\n",
    "\n",
    "for layer in model.layers[:3]:\n",
    "  layer.trainable = False\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data stream and iterate efficiently via tf.data\n",
    "label_dim = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f06c0139e80>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(str(model_path / 'checkpoint_sample_oie'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_token (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_token (InputLayer)       [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_1 (TFDisti ((None, 128, 768),)  66362880    input_token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 128), (None, 459264      tf_distil_bert_model_1[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 66,822,144\n",
      "Trainable params: 459,264\n",
      "Non-trainable params: 66,362,880\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, None, 30522) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 128),  15693312    input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 30522)  3937338     lstm_4[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 19,630,650\n",
      "Trainable params: 19,630,650\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = tf.keras.Model([encoder_inputs,encoder_masks], encoder_states)\n",
    "print(encoder_model.summary())\n",
    "\n",
    "lm_embedding = transformer_model(encoder_inputs, attention_mask=encoder_masks)[0]\n",
    "encoder = tf.keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(lm_embedding)\n",
    "\n",
    "\n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = tf.keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "print(decoder_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    input_mask = np.array([0 if x==0 else 1 for x in input_seq[0]]).reshape(1,128)\n",
    "    states_value = encoder_model.predict((input_seq,input_mask))\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    CLS_id = 101 # tokenizer.encode('[CLS]') to check this\n",
    "    SEP_id = 102\n",
    "    PAD_id = 0 \n",
    "    target_seq[0, 0, CLS_id] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    sentence_ids = []\n",
    "    \n",
    "    # We will only sample from the words we were given in the input, drawing from\n",
    "    # the specific goal of a \"triple extraction\" task w.r.t a sentence.\n",
    "    # without the word restriction, we end up combining extraction with ~paraphrase.\n",
    "    eligible_word_ids = list(input_seq[0])\n",
    "    eligible_word_ids.append(SEP_id)\n",
    "    eligible_word_ids.append(SEP_id)\n",
    "    eligible_word_ids.append(PAD_id)\n",
    "    \n",
    "    while not stop_condition:\n",
    "        sent_wid_set = set(eligible_word_ids)\n",
    "        sent_mask = np.ones(tokenizer.vocab_size)\n",
    "        sent_mask[list(sent_wid_set)] = 0\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq]+states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        word_likelihoods = output_tokens[0, -1, :]\n",
    "        sent_word_likelihoods = np.ma.masked_array(word_likelihoods, mask=sent_mask)\n",
    "        sampled_token_index = np.argmax(sent_word_likelihoods)\n",
    "        sentence_ids.append(sampled_token_index)\n",
    "        # remove from eligible words, as each predicted word can be considered\n",
    "        # a \"used\" element of the original input\n",
    "        eligible_word_ids.remove(sampled_token_index)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_token_index == PAD_id or\n",
    "           len(sentence_ids) > max_target_size):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return tokenizer.decode(sentence_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Input: The meeting was scheduled to start at half past six .\n",
      "\n",
      "Extracted: the meeting [SEP] was [SEP] to start at six [SEP]\n"
     ]
    }
   ],
   "source": [
    "def string_to_triples(text):\n",
    "    text_ids = tokenizer.encode(text, max_length=max_input_size, padding='max_length')\n",
    "    triples = decode_sequence(np.array([text_ids])).replace(' [PAD]','')\n",
    "    print(\"---\")\n",
    "    print(\"Input: %s\"%text)\n",
    "    print(\"\")\n",
    "    print(\"Extracted: %s\"% triples)\n",
    "    \n",
    "    \n",
    "string_to_triples('The meeting was scheduled to start at half past six .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "## Raw model output (6 epochs)\n",
    "\n",
    "Here is a sample of using the **raw** model after **6 epochs** of training on the full data:\n",
    "\n",
    "1. Cuisine\n",
    "> Input: a cake has multiple layers .\n",
    ">\n",
    "> Extracted: a bread [SEP] has [SEP] two pieces [SEP]\n",
    "\n",
    "2. Mathematics\n",
    "> Input: In mathematics, a monomial is, roughly speaking, a polynomial which has only one term.\n",
    ">\n",
    "> Extracted: a broad number [SEP] has [SEP] an infinite number [SEP]\n",
    "\n",
    "3. Biology\n",
    ">Input: A tiger is a cat with stripes .\n",
    ">\n",
    ">Extracted: a brother [SEP] is [SEP] a bear [SEP]\n",
    "\n",
    "4. Social\n",
    "> Input: The meeting was scheduled to start at half past six .\n",
    "> \n",
    "> Extracted: the event [SEP] was [SEP] to be answered on 12 january [SEP]\n",
    "\n",
    "## Sentence-only dictionary\n",
    "\n",
    "Here is the same checkpoint (after 6 epochs) on these examples when we first mask the predictions to only make visibles words that appeared in the original input. Much better!\n",
    "\n",
    "1. Cuisine\n",
    "> Input: a cake has multiple layers .\n",
    ">\n",
    "> Extracted: a cake [SEP] has [SEP] multiple layers [SEP]\n",
    "\n",
    "2. Mathematics\n",
    "> Input: In mathematics, a monomial is, roughly speaking, a polynomial which has only one term.\n",
    ">\n",
    "> Extracted: a monol [SEP] has [SEP] one term [SEP]\n",
    "\n",
    "3. Biology\n",
    ">Input: A tiger is a cat with stripes .\n",
    ">\n",
    ">Extracted: a tiger [SEP] is [SEP] a tiger [SEP]\n",
    "\n",
    "4. Social\n",
    "> Input: The meeting was scheduled to start at half past six .\n",
    "> \n",
    "> Extracted: the meeting [SEP] was [SEP] to start to the six [SEP]\n",
    "\n",
    "### Sentence-only with one-use-per-input-occurrence\n",
    "1. Cuisine\n",
    "> Input: a cake has multiple layers .\n",
    ">\n",
    "> Extracted: a cake [SEP] has [SEP] multiple layers [SEP]\n",
    "\n",
    "2. Mathematics\n",
    "> Input: In mathematics, a monomial is, roughly speaking, a polynomial which has only one term.\n",
    ">\n",
    "> Extracted: a monol [SEP] has [SEP] one term [SEP]\n",
    "\n",
    "3. Biology\n",
    ">Input: A tiger is a cat with stripes .\n",
    ">\n",
    ">Extracted: a tiger [SEP] is [SEP] a [SEP]\n",
    "\n",
    "4. Social\n",
    "> Input: The meeting was scheduled to start at half past six .\n",
    "> \n",
    "> Extracted: the meeting [SEP] was [SEP] to start at six [SEP]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
